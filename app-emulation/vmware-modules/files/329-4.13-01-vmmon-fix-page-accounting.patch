From fadedd9c8a4dd23f74da2b448572df95666dfe12 Mon Sep 17 00:00:00 2001
From: Michal Kubecek <mkubecek@suse.cz>
Date: Sun, 10 Sep 2017 20:30:21 +0200
Subject: [PATCH 5/6] vmmon: fix page accounting

global_page_state() was renamed to global_zone_page_state() by commit
c41f012ade0b ("mm: rename global_page_state to global_zone_page_state").
However, some more changes are needed (and were in fact needed even with
kernels older than 4.14. In particular, several users reported failures
when starting VMs on hosts with 4.13 kernel with message "not enough
physical memory" due to wrong accounting of NR_SLAB_UNRECLAIMABLE pages.

Since commit 385386cff4c6 ("mm: vmstat: move slab statistics from zone to
node counters") in v4.13-rc1, NR_SLAB_UNRECLAIMABLE needs to be used with
global_node_page_state(), in-tree callers were fixed by commit d507e2ebd2c7
("mm: fix global NR_SLAB_.*CLAIMABLE counter reads").

Since commit 599d0c954f91 ("mm, vmscan: move LRU lists to node") in
v4.8-rc1, NR_UNEVICTABLE needs global_node_page_state() rather than
global_page_state().

Since commit 50658e2e04c1 ("mm: move page mapped accounting to the node")
in v4.8-rc1, NR_ANON_PAGES needs global_node_page_state() as well. This was
shortly before it was renamed to NR_ANON_MAPPED but as both got into
mainline in v4.8-rc1, we can do with one #ifdef.

To keep HostIF_EstimateLockedPageLimit() readable, extract the version
dependent calls into inline helpers.
---
 vmmon-only/linux/hostif.c | 44 +++++++++++++++++++++++++++++++++++---------
 1 file changed, 35 insertions(+), 9 deletions(-)

diff --git a/vmmon-only/linux/hostif.c b/vmmon-only/linux/hostif.c
index 78199da..9a94769 100644
--- a/vmmon-only/linux/hostif.c
+++ b/vmmon-only/linux/hostif.c
@@ -79,6 +79,37 @@
 #error CONFIG_HIGH_RES_TIMERS required for acceptable performance
 #endif
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0)
+#   define global_zone_page_state global_page_state
+#endif
+
+static unsigned long get_nr_slab_unreclaimable(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 13, 0)
+   return global_node_page_state(NR_SLAB_UNRECLAIMABLE);
+#else
+   return global_page_state(NR_SLAB_UNRECLAIMABLE);
+#endif
+}
+
+static unsigned long get_nr_unevictable(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)
+   return global_node_page_state(NR_UNEVICTABLE);
+#else
+   return global_page_state(NR_UNEVICTABLE);
+#endif
+}
+
+static unsigned long get_nr_anon_mapped(void)
+{
+ #if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)
+   return global_node_page_state(NR_ANON_MAPPED);
+ #else
+   return global_page_state(NR_ANON_PAGES);
+ #endif
+}
+
 /*
  * Although this is not really related to kernel-compatibility, I put this
  * helper macro here for now for a lack of better place --hpreg
@@ -1516,16 +1547,11 @@ HostIF_EstimateLockedPageLimit(const VMDriver* vm,                // IN
    unsigned int reservedPages = MEMDEFAULTS_MIN_HOST_PAGES;
    unsigned int hugePages = (vm == NULL) ? 0 :
       BYTES_2_PAGES(vm->memInfo.hugePageBytes);
-   unsigned int lockedPages = global_page_state(NR_PAGETABLE) +
-                              global_page_state(NR_SLAB_UNRECLAIMABLE) +
-                              global_page_state(NR_UNEVICTABLE) +
+   unsigned int lockedPages = global_zone_page_state(NR_PAGETABLE) +
+                              get_nr_slab_unreclaimable() +
+                              get_nr_unevictable() +
                               hugePages + reservedPages;
-   unsigned int anonPages =
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 8, 0)
-      global_page_state(NR_ANON_MAPPED);
-#else
-      global_page_state(NR_ANON_PAGES);
-#endif
+   unsigned int anonPages = get_nr_anon_mapped();
    unsigned int swapPages = BYTES_2_PAGES(linuxState.swapSize);
 
    if (anonPages > swapPages) {
-- 
2.14.3

